## Task1：数据分析

爱因斯坦，牛顿之所以伟大因为他们不是一个人在战斗，他们都有着良好氛围的圈子，可以分享讨论观点。所以伟大从参加讨论开始~~~

### 1&emsp;问题讨论

**因为本次任务是基础的数据预处理和数据集划分，所以本次讨论不涉及特征工程和模型相关的。**

**【问题一 缺失数据】**

 - 参考文献：[Python数据分析基础: 数据缺失值处理](https://juejin.im/post/5b5c4e6c6fb9a04f90791e0c)

 - Q1：为什么需要处理缺失值？

   答：就如同一件喜欢的衣服上有洞的情况一样，洞小、洞少情况下舍不得扔，打打补丁不影响美观的情况下还可以继续穿。但若洞大洞多了，没法穿或穿出去被围观，就只能忍痛仍了缺失值的缺失率和重要性，以及不同缺失值的处理方式在一定程度上影响了特征提取、建模和模型训练缺失值太多，可以尝试着直接删除，如果不删除，处理不好，可能会引来噪声缺失值较少，少于某一缺失率时，直接删除又会带来信息的损失，此时可以采取适当的填充方式

 - Q2：缺失率大于多少时应当抛弃该特征？
 
   答：一般为70%,但是还要分析该特征与训练目标的重要程度

 - Q3：缺失值填充有哪些方法？
 
 ![缺失值处理](./fig/1.jpg)

 - Q4：采用各种填充方法的影响或者优缺点？
 
   答：平均值：大大降低数据的方差，即随机性

 - Q5：需要依据什么样的准则去选择合适的方法？
 
   答：
   
   - 删除：如果行和列的缺失达到一定的比例，建议放弃整行或整列数据

   - 补全统计法：对于数值型的数据，如果是连续性，就使用平均值插补，如果是离散性，就使用中位数、众数补足；对于分类型数据，使用类别众数最多的值补足。

   - 模型法：更多时候我们会基于已有的其他字段，将缺失字段作为目标变量进行预测，从而得到最为可能的补全值。如果带有缺失值的列是数值变量，采用回归模型补全；如果是分类变量，则采用分类模型补全。

   - 专家补全：对于少量且具有重要意义的数据记录，专家补足也是非常重要的一种途径。

   - 真值转换：该思路的根本观点是，我们承认缺失值的存在，并且把数据缺失也作为数据分布规律的一部分，这将变量的实际值和缺失值都作为输入维度参与后续数据处理和模型计算。但是变量的实际值可以作为变量值参与模型计算，而缺失值通常无法参与运算，因此需要对缺失值进行真值转换。

   - 很多模型对于缺失值有容忍度或灵活的处理方法，因此在预处理阶段可以不做处理。常见的能够自动处理缺失值的模型包括：KNN、决策树和随机森林、神经网络和朴素贝叶斯、DBSCAN

**【问题二 数据探索】**

 - Q：对于字段较少的情况下经常使用绘图来更直观的观察数据的分布，进而对数据进行针对性的处理；但是再字段量较多的情况下一个一个字段去绘图会比较费时间，那应该用怎么的顺序逻辑对字段进行处理？

   答：我采取的方式是首先去判断哪些字段值重复率较高，这个通过sql语句group by可以直接看出来。其次把数据通过spss对每一个特征进行分析，是绘图还是简单的分析，软件里面都有提供，基本上通过上面两步保证百分之七八十吧，如果仅仅是是在数据探索阶段的话，基本上就完成了

**【问题三 时间序列】**

 - Q：时间序列应该怎么处理？除了提取天数还能做什么处理？
 
   答：通过绘图，查看结果的周期型变化，构造不同的时间维度出来，年月日周小时等；或者也可以自己构造一个时间段出来（聚类）。

**【问题四 异常值和离群值】**

 - Q1：怎样判断离群值以及是否需要删除离群值或怎样替代离群值？（比如一些手动录入过程中出错产生的离群值等）
 
   答：
   
   - 基于统计：BPrule 算法、3倍绝对中位差、分位数等；
   
   - 基于近邻性：基于距离（Index-Based 算法、Nested-Loop 算法、Celi-Based 算法）；基于密度（LOF 算法、LSC算法）等；
   
   - 基于聚类方法、基于分类算法：大部分聚类、分类算法均可
 
 - Q2：为什么要处理离群值？
 
 &emsp;&emsp;答：大多数的参数统计值，如均值、标准差、相关系数等，以及基于这些参数的统计分析，均对离群值高度敏感。

**【问题五 分类数据的编码】**

 - 参考资料：[处理高基数单类别变量的方案](https://mp.weixin.qq.com/s/U93vvFwZ8vSJuswk24yc6w)

 - Q1：对于各种离散型特征，如何进行编码？有哪些方法？以及为啥需要编码？
 
 - Q2：类别数据需要进行不同的编码，那么数值型数据呢？需要做什么处理？

**【问题六 数据不平衡问题】**

 - 参考文献：[机器学习中如何处理不平衡数据？](https://www.jiqizhixin.com/articles/021704)

 - Q：如何处理数据不平衡的问题？(主要是指标签列的类别间不平衡)
 
   答：1. 改变数据量的大小，使类别间变得平衡；2. 不改变数据量大小，设置成本矩阵或代价函数来进行限定。
 
   策略：

   - 扩大数据集。以期得到更多的分布信息；同时扩大数据量后也方便后面的重新采样。
  
   - 尝试新角度理解问题。我们可以把那些小类的样本作为异常点(outliers)，因此该问题便转化为异常点检测(anomaly detection)与变化趋势检测问题(change detection)。异常点检测即是对那些罕见事件进行识别。如通过机器的部件的振动识别机器故障，又如通过系统调用序列识别恶意程序。这些事件相对于正常情况是很少见的。变化趋势检测类似于异常点检测，不同在于其通过检测不寻常的变化趋势来识别。如通过观察用户模式或银行交易来检测用户行为的不寻常改变。

   1. Sampling methods(采样方法)
   
      **增加偏少的类别 - OverSampling过采样**
      
      ① 简单随机重复。
      
      ② SMOTE。在相同边界内，人工造数据。
      
      **减少偏多的类别 - UnderSampling欠采样**
      
      ① 简单随机抽样。
      
      ② 带边界清理。尽量保留分布信息。
      
   2. 引入不同的评价指标
   
      由于正确率太过片面。可以通过G-Mean指标，特别是ROC曲线（接受者工作特征曲线-Receiver Operating Characteristic Curve）和AUC曲线下面积。
   3. Cost-sensitive methods(代价敏感学习方法)
   
      代价敏感学习方法的核心要素是代价矩阵，我们注意到在实际的应用中不同类型的误分类情况导致的代价是不一样的，例如在医疗中，“将病人误疹为健康人”和“将健康人误疹为病人”的代价不同；在信用卡盗用检测中，将盗用误认为正常使用”与“将正常使用识破认为盗用”的代价也不相同。
      
   4. 采用集成学习思想。
   
      把多数类进行划分，然后和少数类组合成多个小的训练集，然后生成学习器，最后再集成。
 
### 2&emsp;笔记

 - 一般而言，在保证测试集和训练集分布一致的情况下，先划分数据集再进行数据预处理（这个保证也有可能是个坑）
 
 - 数据建模时，本质上是计算类别中的距离，离散型特征是否存在距离决定了我们该用什么方式编码
 
 - 欠采样、过采样（上采样、下采样）与 SMOTE

## Task2:特征工程

### 1&emsp;讨论

**【问题一：特征衍生】**
 - Q：特征衍生是否有必要进行？出现的新特征是否有参考价值？依据？

   答：特征衍生的一些常用套路：

   1. 根据**业务理解**对多个特征间做加减乘除处理，比如面积和价格，就可以生成一个单位面积价格

   2. 构建**描述性统计指标**，比如均值 方差 众数

   3. 还可以考虑将**特征构造不同的组别（分箱+onehote）**，比如年龄可以构造成 幼儿 少年 青年 中年 老年等

   判断衍生特征是否是有用的唯一标准是看能不能提升分数

   一是模型最终分数；二也可以通过特征重要度排名。

**【问题二：特征选择】**
 - 参考文献
 
   - [特征工程之特征选择](https://www.cnblogs.com/pinard/p/9032759.html)
 
   - [特征选择 (feature_selection)](https://www.cnblogs.com/stevenlk/p/6543628.html)
   
   - [IV值的计算与使用](https://www.jianshu.com/p/cc4724a373f8)
   
 - Q：特征选择时有过滤法、包装法，集成法，这些方法在应用的时候有没有什么特别的应用场景，还是随机选择？

   答：过滤法有方差筛选、相关系数、卡方检验和互信息等，方差帅选可以剔除方差为0的无用信息；相关系数主要用于监督学习中，设定阈值，选择相关系数较大的特征；卡方检验要求非负特征集，应用场景是衡量特定条件下的分布是否与理论分布一致；户信息偏袒具有较多值的属性；个人觉得在没有什么思路的时候，可以优先使用卡方检验和互信息来做特征选择

   包装法把将要使用的学习器的性能作为特征子集的评价标准，它会选择一个目标函数来一步步的筛选特征，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练，前提是先确定学习模型，个人觉得包装法的优先级可以放过滤法后

   集成法比如随机森林，个人觉得优先级可以放过滤法后，比如先进行方差移除无用特征，再进行集成法(4) 总之任何一种特征选择方法都有其优缺点的，但最终目的都是为了优化模型，将各种方法选出的特征放到模型中去验证，看其对模型的影响

 - Q：IV值可以衡量某个特征对目标的影响程度，根据IV值过滤的特征，这个取值范围该怎么选比较合适，极端值的处理方式？

| Information Value| Predictive Power|
| :---:|:---:|
| < 0.02| useless for prediction|
| 0.02 to 0.1| Weak predictor|
| 0.1 to 0.3 | Medium predictor|
| 0.3 to 0.5 | Strong predictor|
| >0.5 |Suspicious or too good to be true|


 - Q：求IV值遇到极端值时怎么处理？比如说为0该怎么处理。根据随机森林选取特征计算出feature_importances_的值以后，是根据从大到小排列之后又该怎么筛选特征？

   答：极端情况下，当前分组的响应和未响应的比例和样本整体的响应和未响应的比例相等时，IV值为0。用现实语言解释就是根据这个特征并不能区分我们的目标变量。一种就是特征只有一个种类，无意义；另一种就是根据特征去看，全是平均的，毫无区分度

## Task3:模型构建

### 1&emsp;讨论
 - Q：机器学习方法汇总
   
   ![机器学习方法汇总](./fig/3.png)
 
 - Q：为什么交叉验证能解决过拟合？ 
   
   1. 通过分组，多次训练，从某种意义上算变相增加了数据量

   2. cv其实变相的也是学习多个模型，但只是为了最优超参数，最终模型的确定还是最后使用全训练集得到的模型，这就相当于提高模型的泛化能力，降低过拟合。以网格搜索为例，比如你在进行网格搜索时，给定几个超参数，此时会先得到最优超参数，而最终模型的确定是使用该超参数再次进行模型训练。

   3. 算法是有超参数的，采用交叉验证运行不同的超参数，然后取平均值对不同超参数的效果进行对比，比较不同超参数后，可以取出一个较好效果的超参数。在一定情况下避免过拟合
   
 - Q：为什么正则化可以抑制过拟合情况？

   答：过拟合的时候，拟合函数的系数往往非常大。（过拟合就是拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大，在某些很小的去建立，函数值的变化很剧烈，这就意味着函数在某些小区间里的导数值非常大，由于自变量可大可小，所以只有洗漱足够大，才会造成导数值很大），而正则化是通过约束参数的范数，使其不要太大，从而可在一定程度上减少过拟合情况。

### 2&emsp;笔记

 - 降低过拟合的方法
 
   1. 从数据入手，获取更多的训练数据（效果有限）。
   
   2. 降低模型复杂度。在数据样本较少的情况下，模型过于复杂时产生过拟合的主要因素，适当降低模型复杂度可以避免模型拟合过多的采样噪声。如在决策树中降低数的深度、进行剪枝等
   
   3. 正则化方法。给模型参数加上一定的正则约束，如 L2 正则化
   
   4. 集成学习方法。集成学习时把多个模型集成在一起，来降低单一模型的过拟合方法。
   
 - 降低欠拟合的方法
 
   1. 添加新特征。
   
   2. 增加模型的复杂度。
   
   3. 减小正则化系数。
   
  - 拓展
  
    决策树和增强树算法本质上不受多重共线性的影响。当他们决定拆分时，树将只选择一个完全相关的特征。但是， Logistic 回归或线性回归等其他算法并不能解决这个问题，解决方案一般是删除一个完美相关的特征，或是使用降维算法，例如主成分分析（PCA）或是惩罚函数（如 Ridge、Lasso）。共线性检测可以通过使用 numpy 的 corrcoef 函数来完成，而多元共线性使用VIF判断。

